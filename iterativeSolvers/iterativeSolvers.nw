% -*- mode: Noweb; noweb-code-mode: c-mode -*-
The iterativeSolvers structure is written in the header:
<<iterativeSolvers.h>>=
#ifndef __ITERATIVESOLVERS_H__
#define __ITERATIVESOLVERS_H__

#ifndef __CEO_H__
#include "ceo.h"
#endif

#ifndef __AASTATS_H__
#include "aaStats.h"
#endif


struct iterativeSolvers {

  <<parameters>>

  void cg_setup(int n_vector);

  void cg_cleanup(void);

  void cg(float *x, aaStats* A, float *b, int max_it, float* x0);

  //  void info(void);
};
#endif // __ITERATIVESOLVERS_H__
@ The routines are expanded in the source:
<<iterativeSolvers.cu>>=
#include "iterativeSolvers.h"

<<CG setup>>

<<CG cleanup>>

<<conjugate gradient>>

@
The iterativeSolvers structure gather several routines implementing iterative methods to solve the linear system: $Ax=b$.
The setup method is allocated the required memory depending of the iterative method type:
<<CG setup>>=
void iterativeSolvers::cg_setup(int n_vector)
{
  N = n_vector;
  HANDLE_ERROR( cudaMalloc((void**)&d__vectors, sizeof(float)*N*4 ) );
  q = d__vectors;
  x = d__vectors + N;
  r = d__vectors + 2*N;
  p = d__vectors + 3*N;
  cublasCreate(&handle);
}
@ 
The structure parameters are
<<parameters>>=
float *d__vectors, *q, *x, *r, *p;
int N;
cublasHandle_t handle;
cublasStatus_t status;
@
Memory is freed with
<<CG cleanup>>= 
void iterativeSolvers::cg_cleanup(void)
{
  printf("\n@(CEO)>iterativeSolvers: freeing memory!\n");
  HANDLE_ERROR( cudaFree( d__vectors ) );
  cublasDestroy(handle);
}
@ 
The next routine implements the conjugate gradient iterative method:
<<conjugate gradient>>=
  void iterativeSolvers::cg(float *x, aaStats* A, float *b, int max_it, float* x0)
{
  float alpha, beta, gamma;
  int k;
  <<conjugate gradient init>>
  for (k=0;k<max_it;k++) {
    printf("CG IT=%d - ",k);
    <<conjugate gradient loop part 1>>
      if (k<(max_it-1)) {
	  <<conjugate gradient loop part 2>>
      }
  }    
}
@ 
The conjugate gradient algorithm is written:
\begin{itemize}
\item initialization:
  \begin{enumerate}
  \item $r=Ax_0$
<<conjugate gradient init>>=
    A->MVM(r , x0);
@ \item $r_0 = b - r_0$
<<conjugate gradient init>>=
beta = -1;
cublasSscal(handle, N, &beta, r, 1);
alpha = 1;
cublasSaxpy(handle, N, &alpha, b, 1, r, 1);
@ \item $p_0=r_0$
<<conjugate gradient init>>=
cublasScopy(handle, N, r, 1, p, 1);
@ \end{enumerate}
\item loop:
  \begin{enumerate}
  \item $q=Ap_k$
<<conjugate gradient loop part 1>>=
    A->MVM(q , p);
@ \item $\gamma=r_k^Tr_k$
<<conjugate gradient loop part 1>>=
cublasSdot(handle, N, r, 1, r, 1, &gamma);
printf("r norm=%6.2f\n",sqrtf(gamma));
if (gamma==0) break;
@ \item $\alpha_k = {\displaystyle \gamma\over p_K^Tq}$
<<conjugate gradient loop part 1>>=
cublasSdot(handle, N, p, 1, q, 1, &alpha);
alpha = gamma/alpha;
@ \item $x_{k+1} = x_k + \alpha_kp_k$
<<conjugate gradient loop part 1>>=
cublasSaxpy(handle, N, &alpha, p, 1, x, 1);
@ \item $r_{k+1} = r_k - \alpha_kq$
<<conjugate gradient loop part 2>>=
alpha = -alpha;
cublasSaxpy(handle, N, &alpha, q, 1, r, 1);
@ \item $\beta_k = {\displaystyle r_{k+1}^Tr_{k+1} \over \gamma} $
<<conjugate gradient loop part 2>>=
cublasSdot(handle, N, r, 1, r, 1, &beta);
beta = beta/gamma;
@ \item $p_{k+1} = r_{k+1} + \beta_k p_k$
<<conjugate gradient loop part 2>>=
cublasSscal(handle, N, &beta, p, 1);
alpha = 1;
cublasSaxpy(handle, N, &alpha, r, 1, p, 1);
@  \end{enumerate}
 \end{itemize}

The test routine is:
<<iterativeSolvers.bin>>=
#ifndef __CEO_H__
#include "ceo.h"
#endif
#ifndef __SOURCE_H__
#include "source.h"
#endif
#ifndef __ATMOSPHERE_H__
#include "atmosphere.h"
#endif
#ifndef __IMAGING_H__
#include "imaging.h"
#endif
#ifndef __CENTROIDING_H__
#include "centroiding.h"
#endif

#include "iterativeSolvers.h"
#ifndef __AASTATS_H__
#include "aaStats.h"
#endif
#ifndef __PASTATS_H__
#include "paStats.h"
#endif

   //#define N 10
<<PA input>>
// Identity
/* void mvm(float *y, float *x) { */
/*   cublasHandle_t handle; */
/*   cublasCreate(&handle); */
/*   cublasScopy(handle, N, x, 1, y, 1); */
/*   cublasDestroy(handle); */
/* } */
void mvm(aaStats* A, float *y, float *x) {
A->MVM(y,x);
}

// Solving Ax=b
int main( void) {

 <<complete test>>

}
@
In the following a simple test is performed:
<<simple test>>=
float *d__x, *d__b;
HANDLE_ERROR( cudaMalloc((void**)&d__x, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMalloc((void**)&d__b, sizeof(float)*N ) );
float b[N] = {1,2,3,4,5,6,7,8,9,10};
HANDLE_ERROR( cudaMemcpy( d__b, b, N*sizeof(float), cudaMemcpyHostToDevice) );

iterativeSolvers iSolve;
iSolve.cg_setup(N);
iSolve.cg(d__x, mvm, d__b, 5, d__x);
iSolve.cg_cleanup();

float *x;
x = (float *)malloc(sizeof(float)*N);
HANDLE_ERROR( cudaMemcpy( x, d__x, N*sizeof(float), cudaMemcpyDeviceToHost) );

for (int k=0;k<N;k++)
  printf("(%d) %5.1f\n",k,x[k]);

HANDLE_ERROR( cudaFree( d__x ) );
HANDLE_ERROR( cudaFree( d__b ) );
free(x);
@ 
A more complete test:
<<complete test>>=
source src, *d__src;
atmosphere atm;
imaging lenslet_array;
centroiding cog;
aaStats A;
paStats B;
iterativeSolvers iSolve;
stats S;
S.setup();

int N = _N_LENSLET_*2, PS_N_PX, PS_E_N_PX, i, j ,k;
float *d__x, *d__b;
HANDLE_ERROR( cudaMalloc((void**)&d__x, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMalloc((void**)&d__b, sizeof(float)*N ) );

src.setup(ARCSEC(0) , 0, INFINITY);
HANDLE_ERROR( cudaMalloc( (void**)&d__src, sizeof(source)*_N_SOURCE_ ) );
HANDLE_ERROR( cudaMemcpy( d__src, &src,
			  sizeof(source)*_N_SOURCE_ ,
			  cudaMemcpyHostToDevice ) );
        
// Single layer turbulence profile
float altitude[] = {0},
  xi0[] = {1},
  wind_speed[] = {10},
  wind_direction[] = {0};
/*
// GMT 7 layers turbulence profile
float altitude[] = {25, 275, 425, 1250, 4000, 8000, 13000},
xi0[] = {0.1257, 0.0874, 0.0666, 0.3498, 0.2273, 0.0681, 0.0751},
wind_speed[] = {5.6540, 5.7964, 5.8942, 6.6370, 13.2925, 34.8250, 29.4187},
wind_direction[] = {0.0136, 0.1441, 0.2177, 0.5672, 1.2584, 1.6266, 1.7462};
*/
// Atmosphere
atm.setup(0.15,30,altitude,xi0,wind_speed,wind_direction);

// SH WFS
lenslet_array.setup();

// Centroid
cog.setup();

float D = 0.4;
float d = D/N_SIDE_LENSLET;
printf("\nd    =%.4f\n",d);
float delta = d/_N_PX_PUPIL_;
printf("\ndelta=%.4f\n",delta);

A.setup(N_SIDE_LENSLET,&atm,d);
B.setup(N_SIDE_LENSLET,2,&atm,d);

PS_N_PX = N_SIDE_LENSLET*_N_PX_PUPIL_;
atm.get_phase_screen(delta,PS_N_PX,delta,PS_N_PX,d__src,0);
float wf_rms, phase2nm;
phase2nm = 1E9*atm.wavelength/2/PI;
wf_rms = phase2nm*S.std(d__src->phase, _N_PIXEL_);
printf("\n WF RMS: %7.2fnm\n",wf_rms);

float phase_screen[_N_PIXEL_];
HANDLE_ERROR( cudaMemcpy( phase_screen, d__src->phase,
			  sizeof(float)*_N_PIXEL_,
			  cudaMemcpyDeviceToHost ) );
/* for (i=0;i<16;i++) { */
/*   printf("|"); */
/*   for (j=0;j<16;j++) { */
/*     k = i*PS_N_PX + j; */
/*     printf("%+6.4E|",phase_screen[k]); */
/*   }  */
/*   printf("\n"); */
/* } */
FILE *fid;
fid = fopen("phaseScreen.bin","wb");
fwrite(phase_screen,sizeof(float),_N_PIXEL_,fid);
fclose(fid);

lenslet_array.propagate(d__src);
float cxy0 = (_N_PX_PUPIL_ - 1)/2.0;
cog.get_data(lenslet_array.d__frame, cxy0, cxy0);
HANDLE_ERROR( cudaMemcpy( d__b              , cog.d__cx,  
			  _N_LENSLET_*sizeof(float), cudaMemcpyDeviceToDevice) );
HANDLE_ERROR( cudaMemcpy( d__b + _N_LENSLET_, cog.d__cy,  
			  _N_LENSLET_*sizeof(float), cudaMemcpyDeviceToDevice) );

float* b;
b = (float *)malloc(sizeof(float)*N);
HANDLE_ERROR( cudaMemcpy( b, d__b,
			  sizeof(float)*N,
			  cudaMemcpyDeviceToHost ) );
printf("\n   Cx       Cy\n");
for (k=0;k<_N_LENSLET_;k++) {
  printf("%+6.4E  %+6.4E\n",b[k],b[k+_N_LENSLET_]);
}

PS_E_N_PX = 2*N_SIDE_LENSLET + 1;
PS_E_N_PX *= PS_E_N_PX;
int *idx, *d__idx;
idx = (int *)malloc(sizeof(int)*_N_LENSLET_);
k = -1;
printf("\n k   idx\n");
for (i=1;i<2*N_SIDE_LENSLET;i+=2) {
  for (j=1;j<2*N_SIDE_LENSLET;j+=2) {
    idx[++k] = i*(2*N_SIDE_LENSLET + 1) + j;
    printf("(%2d) %2d\n",k,idx[k]);
  }
 }
HANDLE_ERROR( cudaMalloc( (void**)&d__idx, sizeof(int)*_N_LENSLET_ ) );
HANDLE_ERROR( cudaMemcpy( d__idx, idx,
			  sizeof(int)*_N_LENSLET_,
			  cudaMemcpyHostToDevice ) );
float *d__ce;
HANDLE_ERROR( cudaMalloc((void**)&d__ce, sizeof(float)*PS_E_N_PX*2 ) );
HANDLE_ERROR( cudaMemset(d__ce, 0, sizeof(float)*PS_E_N_PX*2 ) );

iSolve.cg_setup(N);

iSolve.cg(d__x, &A, d__b, 10, d__x);

dim3 blockDim(16,16);
dim3 gridDim(N_SIDE_LENSLET/16+1,N_SIDE_LENSLET/16+1);
set_pa_input LLL gridDim,blockDim RRR (d__ce, d__x, d__idx, N_SIDE_LENSLET);
set_pa_input LLL gridDim,blockDim RRR (d__ce + PS_E_N_PX, d__x + _N_LENSLET_, d__idx, N_SIDE_LENSLET);

float *d__phase_est;
HANDLE_ERROR( cudaMalloc((void**)&d__phase_est, sizeof(float)*PS_E_N_PX ) );
B.MVM(d__phase_est,d__ce);

HANDLE_ERROR( cudaFree( d__src) );
atm.cleanup();
lenslet_array.cleanup();
cog.cleanup();
A.cleanup();
B.cleanup();
iSolve.cg_cleanup();
S.cleanup();
HANDLE_ERROR( cudaFree( d__x ) );
HANDLE_ERROR( cudaFree( d__b ) );
HANDLE_ERROR( cudaFree( d__idx ) );
HANDLE_ERROR( cudaFree( d__ce ) );
HANDLE_ERROR( cudaFree( d__phase_est ) );
free(b);
free(idx);
@ 
<<PA input>>=
__global__ void set_pa_input(float *pa_c, float *aa_c, int *idx, int N) 
{
  int i, j, k;
  i = blockIdx.x * blockDim.x + threadIdx.x;
  j = blockIdx.y * blockDim.y + threadIdx.y;
  if ( (i<N) && (j<N) ) {
    k = i*N + j;
    pa_c[idx[k]] = aa_c[k];
  }
}

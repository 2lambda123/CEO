% -*- mode: Noweb; noweb-code-mode: c-mode -*-
[[BTBT]] contains a structure and the routines to implement a block matrix with Toeplitz--Block--Toeplitz blocks.
The BTBT structure is written in the header:
<<BTBT.h>>=
#ifndef __BTBT_H__
#define __BTBT_H__

#ifndef __CEO_H__
#include "ceo.h"
#endif

#ifndef __ATMOSPHERE_H__
#include "atmosphere.h"
#endif

//#define BTBT_DEBUG

struct BTBT {

  <<BTBT parameters>>

  void setup(int n_x);

  void setup(int M_, int N_, int NT_, float2 *d__cov_);

  void cleanup(void);

  void info(void);

  void MVM(float *y, float *x);

};
#endif // __BTBT_H__
@ The routines are expanded in the source file:
<<BTBT.cu>>=
#include "BTBT.h"

<<ordering indices>>

<<MVM input ordering kernel>>

<<MVM complex multiplication>>

<<MVM output ordering kernel>>

<<setup (test)>>

<<setup>>

<<cleanup>>

<<info>>

<< MVM >>
@
A BTBT matrix is a $M\times N$ block matrix with $M$ and $N$ the number of block rows and columns respectively.
Each block is a square matrix of type Toeplitz--Block--Toeplitz meaning that each block contains $N_T\times N_T$ blocks of size $N_T\times N_T$.
Both matrix level are Toeplitz.
Thanks to the peculiar structure of the matrix, there is a total of $MN(2N_T+1)^2$ unique elements, to compare to the number of elements in the full matrix, $MNN_T^4$.
The matrix is entirely defined with $M$, $N$ and $N_T$ and the $(2N_T+1)^2\times MN$ matrix of unique elements.
<<setup>>=
  void BTBT::setup(int M_, int N_, int NT_, float2 *d__cov_)
{
  M = M_;
  N = N_;
  NT = NT_;
  NT2 = NT*NT;
  NU = 2*NT-1;
  NU2 = NU*NU;
  NDFT = NU2;//round_up_to_nhp2(NU2);
  NU_TOTAL = NU2*M*N;
  d__cov = d__cov_;

  ind_size = sizeof(int)*NT2;
  cov_size = sizeof(float2)*NU2*N*M;
  HANDLE_ERROR( cudaMalloc((void**)&d__mu, ind_size ) );
  HANDLE_ERROR( cudaMalloc((void**)&d__xi, ind_size ) );
  HANDLE_ERROR( cudaMalloc((void**)&d__b, N*sizeof(float2)*NU2 ) );
  HANDLE_ERROR( cudaMemset(d__b, 0, N*sizeof(float2)*NU2 ) );
  HANDLE_ERROR( cudaMalloc((void**)&d__c, M*sizeof(float2)*NU2 ) );

  int BATCH = M*N;
  printf("\n@(CEO)>imaging: Creating a 1D covariance FFT plan\n");
  if (cufftPlanMany(&raster_plan, 1, &NDFT,
		    NULL, 1, NDFT,
		    NULL, 1, NDFT,
		    CUFFT_C2C,BATCH) != CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to create plan\n");
    return;
  }
  if (cufftSetCompatibilityMode(raster_plan, CUFFT_COMPATIBILITY_NATIVE)!= CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to set compatibility mode to native\n");
    return;
  }

  printf("\n@(CEO)>imaging: Creating a 1D MVM input FFT plan\n");
  BATCH = N;
  if (cufftPlanMany(&MVM_input_plan, 1, &NDFT,
		    NULL, 1, NDFT,
		    NULL, 1, NDFT,
		    CUFFT_C2C,BATCH) != CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to create plan\n");
    return;
  }
  if (cufftSetCompatibilityMode(MVM_input_plan, CUFFT_COMPATIBILITY_NATIVE)!= CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to set compatibility mode to native\n");
    return;
  }

  printf("\n@(CEO)>imaging: Creating a 1D MVM output FFT plan\n");
  BATCH = M;
  if (cufftPlanMany(&MVM_output_plan, 1, &NDFT,
		    NULL, 1, NDFT,
		    NULL, 1, NDFT,
		    CUFFT_C2C,BATCH) != CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to create plan\n");
    return;
  }
  if (cufftSetCompatibilityMode(MVM_output_plan, CUFFT_COMPATIBILITY_NATIVE)!= CUFFT_SUCCESS) {
    fprintf(stderr,
	    "CUFFT Error: Unable to set compatibility mode to native\n");
    return;
  }

  info();  

  dim3 blockDim(16,16);
  dim3 gridDim( 1+NT/16 , 1+NT/16 );
  ordering LLL gridDim,blockDim RRR(d__mu, d__xi, NT);
#ifdef BTBT_DEBUG
  mu = (unsigned int *)malloc( ind_size );
  HANDLE_ERROR( cudaMemcpy( mu, d__mu, ind_size, cudaMemcpyDeviceToHost) );
  xi = (unsigned int *)malloc( ind_size );
  HANDLE_ERROR( cudaMemcpy( xi, d__xi, ind_size, cudaMemcpyDeviceToHost) );
  printf("\n|( i) mu xi|\n");
  for (int i=0;i<NT2;i++) 
    printf("|(%2d) %2d %2d|\n",i,mu[i],xi[i]);
#endif

  stopwatch tid;
  tid.tic();
  <<evaluate covariance>>
  tid.toc(&cov_eval_et,"Covariance evaluation");
}
@ 
The covariance is computed with
<<evaluate covariance>>=
<<raster covariance FT>>
#ifdef BTBT_DEBUG
cov = (float2*)malloc(cov_size);
HANDLE_ERROR( cudaMemcpy( cov, d__cov, cov_size, cudaMemcpyDeviceToHost) );
printf("\n@(CEO)>BTBT: raster covariance\n");
for (int k=0;k<NDFT;k++) {
  printf("%2d: ", k);
  for (int l=0;l<N*M;l++) {
    printf("%+4.2E %+4.2E.I||", cov[k+l*NDFT].x,cov[k+l*NDFT].y);
  }
  printf("\b\b\n");
 }
#endif
@ The main parameters of [[BTBT]] are displayed with the [[info]] routine:
<<info>>=
void BTBT::info(void)
{
  printf("\n\x1B[1;42m@(CEO)>BTBT:\x1B[;42m\n");
  <<info content>>
  printf("----------------------------------------------------\x1B[0m\n");
}
<<info content>>=
  printf(" . number of outermost blocks    : %dX%d\n",M,N);
  printf(" . size of outermost blocks      : %dX%d\n",NT,NT);
  printf(" . size of innermost blocks      : %dX%d\n",NT,NT);
  n_full = powf(NT,4)*M*N;
  printf(" . DFT length                    : %d\n",NDFT);
  printf(" . full matrix elements #        : %.3E\n",n_full);
  n_comp = M*N*NU2;
  printf(" . compressed matrix elements #  : %.3E\n",n_comp);
  printf(" . compression factor            : %4.0f \n",n_full/n_comp);
  float mb = powf(2,20);
  b_full = n_full*sizeof(float);
  if (b_full>mb)
    printf(" . full matrix storage [MB]      : %6.1f\n",b_full/mb);
  else
    printf(" . full matrix storage [KB]      : %6.1f\n",b_full/1024.0);
b_comp = n_comp*sizeof(float2);
  if (b_comp>mb)
    printf(" . compressed matrix storage [MB]: %6.1f\n",b_comp/mb);
  else
    printf(" . compressed matrix storage [KB]: %6.1f\n",b_comp/1024.0);
  printf(" . compression factor            : %4.0f \n",b_full/b_comp);
@ 
<<cleanup>>=
void BTBT::cleanup(void)
{
  printf("\n@(CEO)>BTBT: freeing memory!\n");
  cufftDestroy(raster_plan);
  cufftDestroy(MVM_input_plan);
  cufftDestroy(MVM_output_plan);
  HANDLE_ERROR( cudaFree( d__mu ) );
  HANDLE_ERROR( cudaFree( d__xi ) );
  HANDLE_ERROR( cudaFree( d__b ) );
  HANDLE_ERROR( cudaFree( d__c ) );
#ifdef BTBT_DEBUG
  free(mu);		
  free(xi);
#endif		
}
@
The parameters of the [[BTBT]] structure are:
<<BTBT parameters>>=
int M, N, NT, NT2, NU, NU2, NDFT, NU_TOTAL, NF, NF2, ind_size, cov_size;
float2 *d__cov, *d__b, *d__c;
float *d__alpha, *d__beta, n_full, n_comp, b_full, b_comp, cov_eval_et;
unsigned int *d__mu, *d__xi;
cufftHandle raster_plan, MVM_input_plan, MVM_output_plan;
#ifdef BTBT_DEBUG
float2 *cov;
unsigned int *mu, *xi;
#endif
@ 
The 1D Fourier transform is now applied to the raster covariance
<<raster covariance FT>>=
if (cufftExecC2C(raster_plan, d__cov, d__cov, CUFFT_FORWARD) 
    != CUFFT_SUCCESS){
  fprintf(stderr,"CUFFT Error: Unable to execute plan\n");
 }
if (cudaThreadSynchronize() != cudaSuccess){
  fprintf(stderr, "Cuda error: Failed to synchronize\n");
 }
@ 
The multiplication $Cs$ are efficiently performed as follows
\begin{enumerate}
\item Construct polynomials $C(t)$ and $s(t)$ from the matrix $C$ and the vector $s$:
  \begin{equation}
    \label{eq:33}
    C(t) = \sum_{i_1=-(N_l-1)}^{N_l-1}\sum_{i_2=-(N_l-1)}^{N_l-1} c_{{i_1}{i_2}}t^{\lambda_{{i_1}{i_2}}},
  \end{equation}
where $c_{{i_1}{i_2}}$ is an entry of $C$ and $\lambda_{{i_1}{i_2}}=(N_l+i_1-1)(2N_l-1)+(N_l+i_2-1)$ and
\begin{equation}
  \label{eq:34}
    S(t) = \sum_{j_1=1}^{N_l}\sum_{j_2=1}^{N_l} s_{{j_1}{j_2}}t^{\mu_{{j_1}{j_2}}},  
\end{equation}
where $\mu_{{j_1}{j_2}}=2N_l(N_l-1)-j_1(2N_l-1)-j_2$.
\item Compute $P(t)=C(t)\times S(t)$ using the Discrete Fourier Transform.
\item The entry $b_{{j_1}{j_2}}$ of the vector $b$ is $b_{{j_1}{j_2}}=p_{\xi_{{j_1}{j_2}}}$, where $\xi_{{j_1}{j_2}}=2N_l(2N_l-1)-(j_1+1)(2N_l-1)-(j_2+1).$
\end{enumerate}
with $(j_1,j_2)\in [0,\dots,[[N]]-1]$.
<<ordering indices>>=
__global__ void ordering(unsigned int *mu, unsigned int *xi, int n)
{
  int j1, j2, k;
  j1 = blockIdx.x * blockDim.x + threadIdx.x;
  j2 = blockIdx.y * blockDim.y + threadIdx.y;
  if ( (j1<n) && (j2<n) ) {
    k = j1*n + j2;
    mu[k] = 2*n*(n-1)-j1*(2*n-1)-j2;
    xi[k] = 2*n*(2*n-1) - (j1+1)*(2*n-1) - (j2+1);
  }
}
@ 
The MVM routine computes the matrix--to--vector multiplication $y=C_{\vec\alpha\cdot\vec\beta}s$.
<< MVM >>=
void BTBT::MVM(float *y, float *x)
{
  /* stopwatch tid; */
  /* tid.tic(); */
  <<MVM STEP 1: input ordering>>
  /* tid.toc("STEP 1"); */
  /* tid.tic(); */
  <<MVM STEP 2: input FT>>
  /* tid.toc("STEP 2"); */
 /* tid.tic(); */
  <<MVM STEP 3: Fourier domain multiplication>>
  /* tid.toc("STEP 3"); */
  /*  HANDLE_ERROR( cudaThreadSynchronize() ); */
  /* float2 *b; */
  /* b = (float2*)malloc(M*sizeof(float2)*NU2); */
  /* HANDLE_ERROR( cudaMemcpy( b, d__c, M*sizeof(float2)*NU2, cudaMemcpyDeviceToHost) ); */
  /* for (int i=0;i<(NU2);i++) { */
  /*   printf("|(%3d) [%+6.4E;%+6.4E]\n",i,b[i].x,b[i].y); */
  /* } */
  /* free(b); */
  /* tid.tic(); */
  <<MVM STEP 4: output FT>>
  /* tid.toc("STEP 4"); */
/* tid.tic(); */
  <<MVM STEP 5: output ordering>>
  /* tid.toc("STEP 5"); */
}
@
The vector $s$ is made of $N$ components of length [[NT2]]: $$s= \left[
\begin{array}{c}
  s_x \\
  s_y
\end{array}
\right]
$$
and lets define another complex vector $b$ also made of two components but of length [[NU2]]: $$b=\left[
\begin{array}{c}
  b_x \\
  b_y
\end{array}
\right]
$$
The matrix--to--vector multiplication $y=C_{\vec\alpha\cdot\vec\beta}s$ is derived through the following steps:
\begin{enumerate}
\item input allocation and ordering:
  \begin{itemize}
  \item the $s_x$ components of $s$ is allocated into the real
    part of the complex vector $b_x$ according to the ordering in
    vector $\mu$ i.e. $b_x[\mu].x=s_x$,
  \item the $s_y$ components of $s$ is allocated into the real part of
    the complex vector $b_y$ according to the ordering in vector $\mu$
    i.e. $b_y[\mu].x=s_y$,
  \end{itemize}
<<MVM STEP 1: input ordering>>=
dim3 blockDim(256,1);
dim3 gridDim( 1+NT2/256,1);
int k;
for (k=0;k<N;k++)
  mvm_input_order LLL gridDim,blockDim RRR (d__b + k*NDFT, NDFT, x + k*NT2, NT2, d__mu);
@  using the kernel:
<<MVM input ordering kernel>>=
__global__ void mvm_input_order(float2 *x_out, int n_x_out, 
                                float *x_in, int n_x_in, 
                                unsigned int *ind) 
{
  int k;
  k = blockIdx.x * blockDim.x + threadIdx.x;
  if (k<n_x_in) {
    x_out[ind[k]].x = x_in[k];
  }
}
@ 
\item the Fourier transform of $b_{(x,y)}$ is computed i.e. $\tilde b_{(x,y)}=\mathcal F [b_{(x,y)}]$,
<<MVM STEP 2: input FT>>=
if (cufftExecC2C(MVM_input_plan, d__b, d__b, CUFFT_FORWARD) 
    != CUFFT_SUCCESS){
  fprintf(stderr,"CUFFT Error: Unable to execute plan forward FT with MVM plan\n");
 }
 HANDLE_ERROR( cudaThreadSynchronize() );
@
\item $\tilde b$ and $\tilde T$ are multiplied element wise i.e. 
$$\tilde c = \tilde b\tilde T =
\left[
\begin{array}{c}
  \tilde b_x.x\tilde T_{xx}.x - \tilde b_x.y\tilde T_{xx}.y + \tilde b_y.x\tilde T_{xy}.x - \tilde b_y.y\tilde T_{xy}.y \\
  \tilde b_x.x\tilde T_{xx}.y + \tilde b_x.y\tilde T_{xx}.x + \tilde b_y.x\tilde T_{xy}.y + \tilde b_y.y\tilde T_{xy}.x 
\end{array}
\right]
$$
<<MVM STEP 3: Fourier domain multiplication>>=
blockDim = dim3(256,1);
gridDim = dim3(ceilf(NDFT/256.0),1);
cpx_mult LLL gridDim,blockDim RRR (d__c, d__cov, d__b, NDFT, M, N);
@ 
using the kernel:
<<MVM complex multiplication>>=
  __global__ void cpx_mult(float2* c, float2 *x1, float2*x2, int n_x, int m_in, int n_in) {
  int k, l, i, j, p, q;
  k = blockIdx.x * blockDim.x + threadIdx.x;
  if (k<n_x) {
    for (j=0;j<m_in;j++) {
      p = k + j*n_x;
      c[p].x = 0.0;
      c[p].y = 0.0;
      for (i=0;i<n_in;i++) {
	l = k + i*n_x;
	q = l + n_in*j*n_x;
	c[p].x += x1[q].x*x2[l].x - x1[q].y*x2[l].y;
	c[p].y += x1[q].x*x2[l].y + x1[q].y*x2[l].x;
      }
    }
    for (i=0;i<n_in;i++) {
      l = k + i*n_x;
      x2[l].x = x2[l].y = 0;
    }
  }
}
@ 
\item the inverse Fourier transform of $\tilde c$ is computed i.e. $c=\mathcal F^{-1} [\tilde c]$,
<<MVM STEP 4: output FT>>=
if (cufftExecC2C(MVM_output_plan, d__c, d__c, CUFFT_INVERSE) 
    != CUFFT_SUCCESS){
  fprintf(stderr,"CUFFT Error: Unable to execute inverse FT with MVM plan\n");
 }
 HANDLE_ERROR( cudaThreadSynchronize() );
@
\item the real part of $c$ is affected into vector $y$ according to the ordering in vector $\xi$ i.e. $y=c[\xi].x$.
<<MVM STEP 5: output ordering>>=
blockDim = dim3(256,1);
gridDim  = dim3( 1+NT2/256,1);
for (k=0;k<M;k++) 
  mvm_output_order LLL gridDim,blockDim RRR (y + k*NT2, NT2, d__c + k*NDFT, NDFT, d__xi);
@  using the kernel:
<<MVM output ordering kernel>>=
__global__ void mvm_output_order(float *x_out, int n_x_out, 
                                float2 *x_in, int n_x_in, 
                                unsigned int *ind) 
{
  int k;
  k = blockIdx.x * blockDim.x + threadIdx.x;
  if (k<n_x_out) {
    x_out[k] = x_in[ind[k]].x/n_x_in;
  }
}
@ \end{enumerate}

New matrix--to--vector multiplication routines are defined for test purposes.
The next routine implement the MVM for an identity matrix
<< MVM (Test 1)>>=
void BTBT::MVM(float *y, float *x)
{
  printf("@(CEO)>BTBT: MVM (Test 1)\n");
  cublasHandle_t handle;
  cublasCreate(&handle);
  cublasScopy(handle, 10, x, 1, y, 1);
  cublasDestroy(handle);
}
<< MVM (Test 2)>>=
void BTBT::MVM(float *y, float *x)
{
  printf("@(CEO)>BTBT: MVM (Test 2)\n");

  cublasHandle_t handle;
  cublasCreate(&handle);
  for (int k=0;k<N;k++)
    cublasSasum(handle, N, x, 1, y+k);
  cublasDestroy(handle);
}
@
We also define a void setup routine:
<<setup (test)>>=
void BTBT::setup(int n_x) { N = n_x; }
